"""
Process the CSV files generated by the routing script.
"""


from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from sys import argv
from urllib.parse import quote, unquote

from gtfs_kit.feed import read_feed
import geopandas as gp
import pandas as pd
import ujson as json


day = argv[1]

assert day in ["wednesday", "saturday", "sunday"]

travel_times_dir = Path(f"data/travel_times_{day}")
travel_times_notrans_dir = Path(f"data/travel_times_{day}_notrans")
target_path = Path(f"data/travel_times_proc_{day}_combine")
target_path.mkdir(exist_ok=True)

files = list(travel_times_dir.glob("*.csv"))
files_notrans = list(travel_times_notrans_dir.glob("*.csv"))

# decode filenames
station_names = [unquote(f.name[:-4]) for f in files]
name_for_file = dict(zip(files, station_names))

feed = read_feed("data/20220829_preprocessed.zip", "m")

# Load Bundesland borders
nw_hi: gp.GeoDataFrame = gp.read_file("data/gemeinden_be_bb_geo.json")

stops_in_nw = feed.get_stops_in_area(nw_hi)
stop_names_in_nw = set(stops_in_nw["stop_name"].unique())

df_stop_locations = feed.stops.groupby(by=["stop_name"], as_index=False).aggregate(
    {
        "stop_lon": "mean",
        "stop_lat": "mean",
    }
)
stop_locations_by_name = {
    row.stop_name: (row.stop_lat, row.stop_lon) for row in df_stop_locations.itertuples()
}
stop_locations_by_id = {
    row.stop_id: (row.stop_lat, row.stop_lon) for row in feed.stops.itertuples()
}

station_names_in_nw = [n for n in station_names if n in stop_names_in_nw]

available_stops = set(station_names_in_nw)


def main():
    executor = ThreadPoolExecutor(max_workers=32)

    results = executor.map(process_stop, sorted(files), sorted(files_notrans))
    results = list(results)
    print("Done: ", len(available_stops))

    dead_stops = set(stop_names_in_nw) - available_stops
    for stop_name in dead_stops:
        pass  # process_dead_stop(stop_name)

    print("Done: ", len(available_stops))


def process_dead_stop(stop_name):
    print(f"Dead stop: {stop_name}")

    stop_info = {
        "name": stop_name,
        "coord": stop_locations_by_name[stop_name],
    }

    stop_data = {
        "stop_info": stop_info,
        "destinations": [],
    }

    stop_name_enc = quote(stop_name, safe="")

    with open(target_path / f"{stop_name_enc}.json", "w", encoding="utf-8") as fp:
        json.dump(stop_data, fp, ensure_ascii=False)

    available_stops.add(stop_name)


def process_stop(file: Path, file_notrans: Path):
    stop_name = name_for_file[file]
    stop_name_enc = file.name[:-4]

    print(".", end="", flush=True)

    df = pd.concat(
        [
            pd.read_csv(file_notrans, dtype={"to_stop_id": str, "from_stop_id": str}),
            pd.read_csv(file, dtype={"to_stop_id": str, "from_stop_id": str}),
        ]
    ).drop_duplicates(subset=["to_stop_name"])

    stop_info = {
        "name": stop_name,
        "coord": stop_locations_by_name[stop_name],
    }

    df = df[df["to_stop_name"] != stop_name]

    destinations = [
        {
            "id": row.to_stop_id,
            "from": row.from_stop_name,
            "name": row.to_stop_name,
            "time": row.travel_time,
            "trans": row.transfers,
            "coord": [row.to_stop_lat, row.to_stop_lon],
        }
        for row in df.itertuples()
    ]

    stop_data = {
        "stop_info": stop_info,
        "destinations": destinations,
    }

    with open(target_path / f"{stop_name_enc}.json", "w", encoding="utf-8") as fp:
        json.dump(stop_data, fp, ensure_ascii=False)


if __name__ == "__main__":
    main()
